import streamlit as st
from PIL import Image
import base64
from io import BytesIO
import requests
import json
import numpy as np
import plotly.graph_objects as go
import csv
import cv2

buffered_img = BytesIO()
st.session_state.img.save(buffered_img, format = 'PNG')
img_base64 = base64.b64encode(buffered_img.getvalue()).decode('utf-8')
# st.write(img_base64)

buffered_depth = BytesIO()
# st.session_state.depthMap.convert('L').save(buffered_depth, format = 'PNG')
st.session_state.depthMap.save(buffered_depth, format = 'PNG')
depth_base64 = base64.b64encode(buffered_depth.getvalue()).decode('utf-8')

buffered_roi = BytesIO()
st.session_state.roi.save(buffered_roi, format = 'PNG')
roi_base64 = base64.b64encode(buffered_roi.getvalue()).decode('utf-8')

# st.write(depth_base64)

st.write(len(img_base64))
st.write(len(depth_base64))
st.write(len(roi_base64))

request_dict = {
    "request_summary": {
        "camera_matrix": st.session_state.cam_json["intrinsics"],
        "images_count": 1
    },
    "full_request_data": {
        "camera_matrix": st.session_state.cam_json["intrinsics"],
        "images": {
            "rgb": [
                img_base64
            ],
            "depth_map": [
                depth_base64
            ],
            "region_of_interest": [
                roi_base64
            ]
        },
        "depthscale": st.session_state.cam_json["depthscale"]
    }
}

request = json.dumps(request_dict)

url = "http://api.open-notify.org/this-api-doesnt-exist"

response = requests.post(url, json = request)
st.write(response.status_code)
if response.status_code == 200:
    st.write(response.json())

# MOVE THIS INTO  STATUS_CODE == 200 EVENTUALLY
with open("C:\\Users\\Bohori\\Trial Data\\test data\\Ql4i\\000049\\000049_ob_incam.txt", "r") as pose_file:
    listed_data = []
    data = csv.reader(pose_file, delimiter=' ')
    for row in data:
        to_append = []
        for i in row:
            to_append.append(float(i))
        #st.write(to_append)
        listed_data.append(to_append)
        print(to_append[0])

# Your 4x4 transformation matrix (from pose estimation)
# T = np.array([
#     [ 0.64412218,  0.76462859, -0.02120633,  0.00107171],
#     [ 0.63528013, -0.55019015, -0.54194945, -0.01981154],
#     [-0.42605767,  0.33560979, -0.84014326,  0.57496655],
#     [ 0.0,         0.0,         0.0,         1.0       ]
# ])
T = np.array(listed_data)
R = T[:3, :3]
K = np.array(st.session_state.intrinsics)

## CODE GENERATED BY CHATGPT TO BACK CALCULATE DIMENSIONS OF CUBE
import numpy as np

def invert_intrinsics(K):
    return np.linalg.inv(K)

def backproject_pixel_to_camera_ray(K_inv, pixel):
    """
    pixel: [u, v, 1]
    This is the format in which pixel is expected to be in 
    """
    return K_inv @ pixel

def calculate_object_size(K, bbox_pixels, transformation_matrix):
    """
    K: 3x3 camera intrinsics matrix (numpy array)
    bbox_pixels: (u1, v1, u2, v2) bounding box corners in pixels
    transformation_matrix: 4x4 numpy array (object pose in camera coords)
    """
    # Invert camera intrinsics
    K_inv = invert_intrinsics(K)

    # Extract bounding box corners
    u1, v1, u2, v2 = bbox_pixels
    pixel_tl = np.array([u1, v1, 1])
    pixel_br = np.array([u2, v2, 1])

    # Back-project pixels to camera rays
    ray_tl = backproject_pixel_to_camera_ray(K_inv, pixel_tl)
    ray_br = backproject_pixel_to_camera_ray(K_inv, pixel_br)

    # Depth of object = translation along Z axis
    depth = transformation_matrix[2, 3]  # assuming standard camera coords, Z forward

    # Calculate 3D points in camera coordinates
    P_tl = ray_tl * depth
    P_br = ray_br * depth

    # Calculate real-world width and height
    width = abs(P_br[0] - P_tl[0])
    height = abs(P_br[1] - P_tl[1])

    # For depth, you can estimate or fix a value
    # For example, approximate depth as object’s Z size or a fraction of width
    depth_size = width  # or some other heuristic

    return width, height, depth_size

# Example usage:

# Bounding box pixels from user image (example values)
contours, _ = cv2.findContours(np.array(st.session_state.roi), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Loop through contours
for contour in contours:
    x, y, w, h = cv2.boundingRect(contour)
    # cv2.rectangle(mask, (x, y), (x + w, y + h), 128, 2)  # just an example visualization

# x, y, w, h = cv2.boundingRect(contour)
u1, v1 = x, y
u2, v2 = x + w, y + h

bbox_pixels = (u1, v1, u2, v2) # FIGURE OUT HOW

width, height, depth = calculate_object_size(K, bbox_pixels, T)

print(f"Width: {width}, Height: {height}, Depth: {depth}")

# Now you can apply these to your cube mesh in your 3D framework,
# e.g., cube.scale = (width, height, depth)
# cube.apply_transform(transformation_matrix)

print('checkpoint 1')
# Define a unit cube centered at origin
def get_points(w, h ,d):
    return np.array([
        [-w, -h, -d],
        [ w, -h, -d],
        [ w,  h, -d],
        [-w,  h, -d],
        [-w, -h,  d],
        [ w, -h,  d],
        [ w,  h,  d],
        [-w,  h,  d]
    ])

# Transform cube using 4x4 pose matrix
def transform_cube(cube_pts, T):
    cube_hom = np.hstack((cube_pts, np.ones((len(cube_pts), 1))))  # (8, 4)
    transformed = (T @ cube_hom.T).T
    return transformed[:, :3]

# Cube faces (by vertex index)
faces = [
    [0, 1, 2, 3], [4, 5, 6, 7],
    [0, 1, 5, 4], [2, 3, 7, 6],
    [1, 2, 6, 5], [0, 3, 7, 4]
]

# Get cube and apply transformation
cube = get_points(width/2, height/2, depth/2)
cube_transformed = transform_cube(cube, T)

# Create Plotly figure
fig = go.Figure()

# Draw cube faces
for face in faces:
    x = [cube_transformed[i][0] for i in face + [face[0]]]
    y = [cube_transformed[i][1] for i in face + [face[0]]]
    z = [cube_transformed[i][2] for i in face + [face[0]]]
    fig.add_trace(go.Scatter3d(
        x=x, y=y, z=z, mode='lines',
        line=dict(color='orange', width=4),
        showlegend=False
    ))

print('came out of faces loop; checkpoint 2')
# Draw pose axes
origin = T[:3, 3]
scale = 0.1
colors = ['red', 'green', 'blue']
for i in range(3):  # x, y, z axes
    axis_end = origin + R[:, i] * scale
    fig.add_trace(go.Scatter3d(
        x=[origin[0], axis_end[0]],
        y=[origin[1], axis_end[1]],
        z=[origin[2], axis_end[2]],
        mode='lines',
        line=dict(color=colors[i], width=6),
        name=f'{colors[i]}-axis'
    ))

# Layout
fig.update_layout(
    scene=dict(aspectmode='data'),
    title='Cube at Pose (Defined by 4×4 Matrix)',
    margin=dict(l=0, r=0, t=40, b=0),
)

# Show in Streamlit
st.plotly_chart(fig)

print('first plottting done; checkpoint 3')
## THIS IS OPTION 2
# Example image
image = cv2.imread('C:\\Users\\Bohori\\000049_rgb.png')

# Example keypoints (x, y)
keypoints = [(100, 200), (150, 250), (200, 300), (250, 350)]

# Example skeleton connections (pairs of indices)
skeleton = [(0,1), (1,2), (2,3)]

# Draw keypoints
for x, y in keypoints:
    cv2.circle(image, (int(x), int(y)), 5, (0,255,0), -1)

print('checkpoint 4')

# Draw skeleton
for pt1_idx, pt2_idx in skeleton:
    pt1 = keypoints[pt1_idx]
    pt2 = keypoints[pt2_idx]
    cv2.line(image, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (255,0,0), 2)

print('checkpoint 5')

# Suppose you have rotation and translation matrices for pose axes

# Camera matrix (intrinsics) example
dist_coeffs = np.zeros(5)  # assume no distortion

# 3D axes points
axes_3d = np.float32([[0,0,0], [0.1,0,0], [0,0.1,0], [0,0,0.1]])

# Project axes points
rvec, _ = cv2.Rodrigues(R)
imgpts, _ = cv2.projectPoints(axes_3d, rvec, T, K, dist_coeffs)

origin = tuple(imgpts[0].ravel().astype(int))
x_axis = tuple(imgpts[1].ravel().astype(int))
y_axis = tuple(imgpts[2].ravel().astype(int))
z_axis = tuple(imgpts[3].ravel().astype(int))

# Draw axes
cv2.line(image, origin, x_axis, (0,0,255), 3)  # X axis in red
cv2.line(image, origin, y_axis, (0,255,0), 3)  # Y axis in green
cv2.line(image, origin, z_axis, (255,0,0), 3)  # Z axis in blue

cv2.imshow('Pose Overlay', image)
cv2.waitKey(0)
cv2.destroyAllWindows()
print('done')