import streamlit as st
from PIL import Image
import base64
from io import BytesIO
import requests
import json
import numpy as np
import plotly.graph_objects as go
import csv
import cv2
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

buffered_img = BytesIO()
st.session_state.img.save(buffered_img, format = 'PNG')
img_base64 = base64.b64encode(buffered_img.getvalue()).decode('utf-8')

buffered_depth = BytesIO()
st.session_state.depthMap.save(buffered_depth, format = 'PNG')
depth_base64 = base64.b64encode(buffered_depth.getvalue()).decode('utf-8')

buffered_roi = BytesIO()
st.session_state.roi.save(buffered_roi, format = 'PNG')
roi_base64 = base64.b64encode(buffered_roi.getvalue()).decode('utf-8')

# st.write(depth_base64)

st.write(len(img_base64))
st.write(len(depth_base64))
st.write(len(roi_base64))

request_dict = {
    "request_summary": {
        "camera_matrix": st.session_state.cam_json["intrinsics"],
        "images_count": 1
    },
    "full_request_data": {
        "camera_matrix": st.session_state.cam_json["intrinsics"],
        "images": {
            "rgb": [
                img_base64
            ],
            "depth_map": [
                depth_base64
            ],
            "region_of_interest": [
                roi_base64
            ]
        },
        "depthscale": st.session_state.cam_json["depthscale"]
    }
}

request = json.dumps(request_dict)

url = "http://api.open-notify.org/this-api-doesnt-exist"

response = requests.post(url, json = request)
st.write(response.status_code)
if response.status_code == 200:
    st.write(response.json())

# MOVE THIS INTO STATUS_CODE == 200 EVENTUALLY
with open("C:\\Users\\Bohori\\Trial Data\\test data\\Ql4i\\000049\\000049_ob_incam.txt", "r") as pose_file:
    listed_data = []
    data = csv.reader(pose_file, delimiter=' ')
    for row in data:
        to_append = []
        for i in row:
            to_append.append(float(i))
        #st.write(to_append)
        listed_data.append(to_append)
        print(to_append[0])

T = np.array(listed_data)
R = T[:3, :3]
K = np.array(st.session_state.intrinsics)

## CODE GENERATED BY CHATGPT TO BACK CALCULATE DIMENSIONS OF CUBE

def invert_intrinsics(K):
    return np.linalg.inv(K)

def backproject_pixel_to_camera_ray(K_inv, pixel):
    """
    pixel: [u, v, 1]
    This is the format in which pixel is expected to be in 
    """
    return K_inv @ pixel

def calculate_object_size(K, bbox_pixels, transformation_matrix):
    """
    K: 3x3 camera intrinsics matrix (numpy array)
    bbox_pixels: (u1, v1, u2, v2) bounding box corners in pixels
    transformation_matrix: 4x4 numpy array (object pose in camera coords)
    """
    # Invert camera intrinsics
    K_inv = invert_intrinsics(K)

    # Extract bounding box corners
    u1, v1, u2, v2 = bbox_pixels
    pixel_tl = np.array([u1, v1, 1])
    pixel_br = np.array([u2, v2, 1])

    # Back-project pixels to camera rays
    ray_tl = backproject_pixel_to_camera_ray(K_inv, pixel_tl)
    ray_br = backproject_pixel_to_camera_ray(K_inv, pixel_br)

    # Depth of object = translation along Z axis
    depth = transformation_matrix[2, 3]  # assuming standard camera coords, Z forward

    # Calculate 3D points in camera coordinates
    P_tl = ray_tl * depth
    P_br = ray_br * depth

    # Calculate real-world width and height
    width = abs(P_br[0] - P_tl[0])
    height = abs(P_br[1] - P_tl[1])

    # For depth, you can estimate or fix a value
    # For example, approximate depth as object’s Z size or a fraction of width
    depth_size = width  # or some other heuristic

    return width, height, depth_size

# Example usage:

# Bounding box pixels from user image (example values)
contours, _ = cv2.findContours(np.array(st.session_state.roi), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

# Loop through contours
for contour in contours:
    x, y, w, h = cv2.boundingRect(contour)
    # cv2.rectangle(mask, (x, y), (x + w, y + h), 128, 2)  # just an example visualization

# x, y, w, h = cv2.boundingRect(contour)
u1, v1 = x, y
u2, v2 = x + w, y + h

bbox_pixels = (u1, v1, u2, v2)

width, height, depth = calculate_object_size(K, bbox_pixels, T)

print(f"Width: {width}, Height: {height}, Depth: {depth}")

# Now you can apply these to your cube mesh in your 3D framework,
# e.g., cube.scale = (width, height, depth)
# cube.apply_transform(transformation_matrix)

print('checkpoint 1')
# Define a unit cube centered at origin
# def get_points(w, h ,d):
#     return np.array([
#         [-w, -h, -d],
#         [ w, -h, -d],
#         [ w,  h, -d],
#         [-w,  h, -d],
#         [-w, -h,  d],
#         [ w, -h,  d],
#         [ w,  h,  d],
#         [-w,  h,  d]
#     ])

# # Transform cube using 4x4 pose matrix
# def transform_cube(cube_pts, T):
#     cube_hom = np.hstack((cube_pts, np.ones((len(cube_pts), 1))))  # (8, 4)
#     transformed = (T @ cube_hom.T).T
#     return transformed[:, :3]

# # Cube faces (by vertex index)
# faces = [
#     [0, 1, 2, 3], [4, 5, 6, 7],
#     [0, 1, 5, 4], [2, 3, 7, 6],
#     [1, 2, 6, 5], [0, 3, 7, 4]
# ]

# # Get cube and apply transformation
# cube = get_points(width/2, height/2, depth/2)
# cube_transformed = transform_cube(cube, T)

# # Create Plotly figure
# fig = go.Figure()

# # Draw cube faces
# for face in faces:
#     x = [cube_transformed[i][0] for i in face + [face[0]]]
#     y = [cube_transformed[i][1] for i in face + [face[0]]]
#     z = [cube_transformed[i][2] for i in face + [face[0]]]
#     fig.add_trace(go.Scatter3d(
#         x=x, y=y, z=z, mode='lines',
#         line=dict(color='orange', width=4),
#         showlegend=False
#     ))

# print('came out of faces loop; checkpoint 2')
# # Draw pose axes
# origin = T[:3, 3]
# scale = 0.1
# colors = ['red', 'green', 'blue']
# for i in range(3):  # x, y, z axes
#     axis_end = origin + R[:, i] * scale
#     fig.add_trace(go.Scatter3d(
#         x=[origin[0], axis_end[0]],
#         y=[origin[1], axis_end[1]],
#         z=[origin[2], axis_end[2]],
#         mode='lines',
#         line=dict(color=colors[i], width=6),
#         name=f'{colors[i]}-axis'
#     ))

# # Layout
# fig.update_layout(
#     scene=dict(aspectmode='data'),
#     title='Cube at Pose (Defined by 4×4 Matrix)',
#     margin=dict(l=0, r=0, t=40, b=0),
# )

# # Show in Streamlit
# st.plotly_chart(fig)

# print('first plottting done; checkpoint 3')
## THIS IS OPTION 2
# Example image
image = cv2.imread('C:\\Users\\Bohori\\000049_rgb.png')

# Example keypoints (x, y)
# keypoints = [(100, 200), (150, 250), (200, 300), (250, 350)]


## FOR 2 DIMENSIONAL OVERLAY
# keypoints = [
#     (u1, v1),  # top-left
#     (u2, v1),  # top-right
#     (u2, v2),  # bottom-right
#     (u1, v2),  # bottom-left
# ]

# # Example skeleton connections (pairs of indices)
# skeleton = [(0,1), (1,2), (2,3), (3,0)]

## FOR THREE DIMENSIONAL OVERLAY
# Fake depth offset (in pixels)
dz = 20

# Bottom face
p0 = (u1, v1)
p1 = (u2, v1)
p2 = (u2, v2)
p3 = (u1, v2)

# Top face (offset by dz in both directions for fake 3D)
p4 = (u1 + dz, v1 - dz)
p5 = (u2 + dz, v1 - dz)
p6 = (u2 + dz, v2 - dz)
p7 = (u1 + dz, v2 - dz)

keypoints = [p0, p1, p2, p3, p4, p5, p6, p7]

skeleton = [
    (0, 1), (1, 2), (2, 3), (3, 0),  # bottom rectangle
    (4, 5), (5, 6), (6, 7), (7, 4),  # top rectangle
    (0, 4), (1, 5), (2, 6), (3, 7),  # vertical edges
]

# Draw keypoints
# for x, y in keypoints:
#     cv2.circle(image, (int(x), int(y)), 5, (0,255,0), -1)

# Draw skeleton
for pt1_idx, pt2_idx in skeleton:
    pt1 = keypoints[pt1_idx]
    pt2 = keypoints[pt2_idx]
    cv2.line(image, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (0,255,0), 2)

dist_coeffs = np.zeros(5)  # assume no distortion

# 3D axes points
axes_3d = np.float32([[0,0,0], [0.1,0,0], [0,0.1,0], [0,0,0.1]])

# Project axes points
rvec, _ = cv2.Rodrigues(R)
# imgpts, _ = cv2.projectPoints(axes_3d, rvec, T[:3,3], K, dist_coeffs)
imgpts, _ = cv2.projectPoints(axes_3d, rvec, T[:3,3], K, None)

origin = tuple(imgpts[0].ravel().astype(int))
x_axis = tuple(imgpts[1].ravel().astype(int))
y_axis = tuple(imgpts[2].ravel().astype(int))
z_axis = tuple(imgpts[3].ravel().astype(int))

# Draw axes
cv2.line(image, origin, x_axis, (255,0,0), 3)  # X axis in red
cv2.line(image, origin, y_axis, (0,255,0), 3)  # Y axis in green
cv2.line(image, origin, z_axis, (0,0,255), 3)  # Z axis in blue

st.image(image)